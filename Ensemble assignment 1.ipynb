{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f64c147",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dd47f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An ensemble technique in machine learning is a method that combines predictions from multiple individual models to\\nimprove overall predictive performance and reduce the risk of overfitting. It leverages the diversity of multiple models\\nto make more accurate and robust predictions than any single model on its own. Popular ensemble methods include\\nbagging (e.g., Random Forests), boosting (e.g., AdaBoost), and stacking.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"An ensemble technique in machine learning is a method that combines predictions from multiple individual models to\n",
    "improve overall predictive performance and reduce the risk of overfitting. It leverages the diversity of multiple models\n",
    "to make more accurate and robust predictions than any single model on its own. Popular ensemble methods include\n",
    "bagging (e.g., Random Forests), boosting (e.g., AdaBoost), and stacking.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a01432",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f9c761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ensemble techniques are used in machine learning for several key reasons:\\n\\n1. Improved Predictive Performance: Ensembles often outperform individual models by leveraging the collective wisdom \\nof multiple models, resulting in more accurate and reliable predictions.\\n\\n2. Reduction of Overfitting: Combining multiple models with different strengths and weaknesses can help reduce \\noverfitting because errors in one model can be compensated for by others, leading to a more robust and generalizable model.\\n\\n3. Increased Robustness: Ensembles are less sensitive to noise and outliers in the data, making them more robust \\nin real-world scenarios where data can be noisy or incomplete.\\n\\n4. Versatility: Ensembles can be applied to various machine learning algorithms and problems, making them a\\nversatile tool in a data scientist's toolkit.\\n\\n5. Interpretability: Some ensemble techniques, like Random Forests, provide feature importance scores, helping to \\nunderstand the importance of different features in making predictions.\\n\\n6. Handling Complex Relationships: Ensembles can capture complex relationships in data by combining different modeling \\napproaches, including linear and nonlinear methods.\\n\\n7. State-of-the-Art Performance: In many machine learning competitions and real-world applications, ensemble methods \\nhave consistently achieved state-of-the-art performance and are considered best practices.\\n\\nOverall, ensemble techniques are a valuable strategy for improving the accuracy, reliability, and generalization \\ncapabilities of machine learning models, making them a popular choice in the field.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ensemble techniques are used in machine learning for several key reasons:\n",
    "\n",
    "1. Improved Predictive Performance: Ensembles often outperform individual models by leveraging the collective wisdom \n",
    "of multiple models, resulting in more accurate and reliable predictions.\n",
    "\n",
    "2. Reduction of Overfitting: Combining multiple models with different strengths and weaknesses can help reduce \n",
    "overfitting because errors in one model can be compensated for by others, leading to a more robust and generalizable model.\n",
    "\n",
    "3. Increased Robustness: Ensembles are less sensitive to noise and outliers in the data, making them more robust \n",
    "in real-world scenarios where data can be noisy or incomplete.\n",
    "\n",
    "4. Versatility: Ensembles can be applied to various machine learning algorithms and problems, making them a\n",
    "versatile tool in a data scientist's toolkit.\n",
    "\n",
    "5. Interpretability: Some ensemble techniques, like Random Forests, provide feature importance scores, helping to \n",
    "understand the importance of different features in making predictions.\n",
    "\n",
    "6. Handling Complex Relationships: Ensembles can capture complex relationships in data by combining different modeling \n",
    "approaches, including linear and nonlinear methods.\n",
    "\n",
    "7. State-of-the-Art Performance: In many machine learning competitions and real-world applications, ensemble methods \n",
    "have consistently achieved state-of-the-art performance and are considered best practices.\n",
    "\n",
    "Overall, ensemble techniques are a valuable strategy for improving the accuracy, reliability, and generalization \n",
    "capabilities of machine learning models, making them a popular choice in the field.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16dcb4",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7eb8a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique that aims to \\nimprove the accuracy and stability of predictive models. It works by creating multiple subsets of the original \\ntraining data through a process called bootstrapping (random sampling with replacement) and training a separate \\nbase model (often a decision tree) on each of these subsets. The key steps in bagging are as follows:\\n\\n1. **Bootstrap Sampling**: Randomly select multiple subsets (with replacement) from the original training data.\\nEach subset is typically of the same size as the original dataset.\\n\\n2. **Base Model Training**: Train a base model (e.g., a decision tree) on each of the bootstrap samples independently. \\nEach base model learns from a slightly different perspective due to the variations in the training data.\\n\\n3. **Aggregation**: Combine the predictions of all base models to make a final prediction. For regression tasks, this \\nis often done by averaging the predictions, while for classification tasks, it's typically done by majority voting (i.e., choosing the class that receives the most votes from the base models).\\n\\nThe main benefits of bagging are:\\n\\n- **Reduced Variance**: Since each base model is trained on a different subset of data, they tend to make different \\nerrors. By averaging or combining their predictions, bagging reduces the variance in the final prediction, making it more\\nstable and less prone to overfitting.\\n\\n- **Improved Generalization**: Bagging tends to improve the model's ability to generalize to unseen data, resulting\\nin better predictive performance on test datasets.\\n\\nOne of the most popular bagging algorithms is the Random Forest, which combines bagging with decision trees. Random Forests \\nare known for their robustness, accuracy, and resistance to overfitting, and they have found widespread use in various\\nmachine learning applications.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique that aims to \n",
    "improve the accuracy and stability of predictive models. It works by creating multiple subsets of the original \n",
    "training data through a process called bootstrapping (random sampling with replacement) and training a separate \n",
    "base model (often a decision tree) on each of these subsets. The key steps in bagging are as follows:\n",
    "\n",
    "1. **Bootstrap Sampling**: Randomly select multiple subsets (with replacement) from the original training data.\n",
    "Each subset is typically of the same size as the original dataset.\n",
    "\n",
    "2. **Base Model Training**: Train a base model (e.g., a decision tree) on each of the bootstrap samples independently. \n",
    "Each base model learns from a slightly different perspective due to the variations in the training data.\n",
    "\n",
    "3. **Aggregation**: Combine the predictions of all base models to make a final prediction. For regression tasks, this \n",
    "is often done by averaging the predictions, while for classification tasks, it's typically done by majority voting (i.e., choosing the class that receives the most votes from the base models).\n",
    "\n",
    "The main benefits of bagging are:\n",
    "\n",
    "- **Reduced Variance**: Since each base model is trained on a different subset of data, they tend to make different \n",
    "errors. By averaging or combining their predictions, bagging reduces the variance in the final prediction, making it more\n",
    "stable and less prone to overfitting.\n",
    "\n",
    "- **Improved Generalization**: Bagging tends to improve the model's ability to generalize to unseen data, resulting\n",
    "in better predictive performance on test datasets.\n",
    "\n",
    "One of the most popular bagging algorithms is the Random Forest, which combines bagging with decision trees. Random Forests \n",
    "are known for their robustness, accuracy, and resistance to overfitting, and they have found widespread use in various\n",
    "machine learning applications.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f268f43",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ebbee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting is an ensemble machine learning technique that aims to improve the performance of weak learners\\n(models that are slightly better than random guessing) by combining them into a strong learner. Unlike \\nbagging, which creates multiple models independently, boosting builds a sequence of models iteratively, \\nwith each model focusing on correcting the errors of the previous ones. The key steps in boosting are as follows:\\n\\n1. **Initial Model**: A weak base model (e.g., a decision stump, which is a shallow decision tree with only one split) \\nis trained on the entire dataset.\\n\\n2. **Weighted Data**: Each data point in the training set is assigned a weight. Initially, all weights are set equally.\\n\\n3. **Iterative Training**: Boosting operates in iterations, where each iteration focuses on the misclassified data points \\nfrom the previous iteration. It increases the importance of the misclassified points while decreasing the importance of \\ncorrectly classified points.\\n\\n4. **Model Combination**: At each iteration, a new weak model is trained on the weighted data. The predictions of these \\nmodels are combined in a weighted manner to form the final prediction.\\n\\n5. **Updating Weights**: After each iteration, the weights of the data points are updated. Misclassified points are\\ngiven higher weights, and correctly classified points are given lower weights. This process emphasizes the examples that are difficult to classify.\\n\\n6. **Stopping Criterion**: Boosting continues for a fixed number of iterations or until a stopping criterion \\n(e.g., a maximum number of weak models or a certain level of accuracy) is met.\\n\\nThe main benefits of boosting are:\\n\\n- **Improved Accuracy**: Boosting can significantly improve the accuracy of weak learners, often leading to a strong overall\\nmodel.\\n\\n- **Handling Complex Patterns**: It can capture complex patterns in the data by iteratively focusing on the examples that \\nare difficult to classify.\\n\\n- **Reduced Bias**: Boosting reduces bias by iteratively adjusting the model to correct its errors, which can lead to \\nbetter generalization.\\n\\nPopular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost, among others. \\nThese algorithms differ in their specific techniques for weighting data points, building weak models, and combining predictions, but they all follow the general boosting framework to enhance model performance.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Boosting is an ensemble machine learning technique that aims to improve the performance of weak learners\n",
    "(models that are slightly better than random guessing) by combining them into a strong learner. Unlike \n",
    "bagging, which creates multiple models independently, boosting builds a sequence of models iteratively, \n",
    "with each model focusing on correcting the errors of the previous ones. The key steps in boosting are as follows:\n",
    "\n",
    "1. **Initial Model**: A weak base model (e.g., a decision stump, which is a shallow decision tree with only one split) \n",
    "is trained on the entire dataset.\n",
    "\n",
    "2. **Weighted Data**: Each data point in the training set is assigned a weight. Initially, all weights are set equally.\n",
    "\n",
    "3. **Iterative Training**: Boosting operates in iterations, where each iteration focuses on the misclassified data points \n",
    "from the previous iteration. It increases the importance of the misclassified points while decreasing the importance of \n",
    "correctly classified points.\n",
    "\n",
    "4. **Model Combination**: At each iteration, a new weak model is trained on the weighted data. The predictions of these \n",
    "models are combined in a weighted manner to form the final prediction.\n",
    "\n",
    "5. **Updating Weights**: After each iteration, the weights of the data points are updated. Misclassified points are\n",
    "given higher weights, and correctly classified points are given lower weights. This process emphasizes the examples that are difficult to classify.\n",
    "\n",
    "6. **Stopping Criterion**: Boosting continues for a fixed number of iterations or until a stopping criterion \n",
    "(e.g., a maximum number of weak models or a certain level of accuracy) is met.\n",
    "\n",
    "The main benefits of boosting are:\n",
    "\n",
    "- **Improved Accuracy**: Boosting can significantly improve the accuracy of weak learners, often leading to a strong overall\n",
    "model.\n",
    "\n",
    "- **Handling Complex Patterns**: It can capture complex patterns in the data by iteratively focusing on the examples that \n",
    "are difficult to classify.\n",
    "\n",
    "- **Reduced Bias**: Boosting reduces bias by iteratively adjusting the model to correct its errors, which can lead to \n",
    "better generalization.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost, among others. \n",
    "These algorithms differ in their specific techniques for weighting data points, building weak models, and combining predictions, but they all follow the general boosting framework to enhance model performance.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e77df8",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab7dbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ensemble techniques offer several benefits in the context of machine learning:\\n\\n1. **Improved Predictive Performance**: Ensembles often achieve higher accuracy and better generalization compared to individual models. They harness the wisdom of multiple models to make more accurate predictions.\\n\\n2. **Reduction of Overfitting**: Combining diverse models helps mitigate overfitting. Errors or biases in one model can be offset by others, leading to a more robust and less prone-to-overfitting model.\\n\\n3. **Enhanced Robustness**: Ensembles are less sensitive to noisy or outlier data points. They can provide more stable predictions in real-world scenarios where data quality may vary.\\n\\n4. **Versatility**: Ensemble methods can be applied to various machine learning algorithms and problem types. This versatility makes them a valuable tool in many different domains.\\n\\n5. **Interpretability**: Some ensemble methods, like Random Forests, offer insights into feature importance, helping understand which features are crucial for making predictions.\\n\\n6. **State-of-the-Art Performance**: Ensembles have consistently delivered top-tier performance in machine learning competitions and real-world applications, making them a go-to choice when high accuracy is required.\\n\\n7. **Handling Complex Relationships**: Ensembles can capture complex patterns and relationships in data by combining different modeling approaches, including both linear and nonlinear methods.\\n\\n8. **Risk Diversification**: In financial or risk modeling, ensembles can help diversify risk by considering multiple models' outputs, which can be crucial for decision-making.\\n\\n9. **Adaptability**: Ensemble techniques can be customized to suit specific needs. For instance, one can create an ensemble with different types of base models or vary the ensemble size to balance complexity and performance.\\n\\n10. **Scalability**: Ensembles can be distributed across multiple machines or processors, making them suitable for large-scale and parallelized machine learning tasks.\\n\\nOverall, ensemble techniques are a powerful tool for improving model performance, reducing overfitting, and increasing the robustness of machine learning models in various applications. However, it's essential to choose the right ensemble method and tune its parameters carefully to achieve the best results for a specific problem.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ensemble techniques offer several benefits in the context of machine learning:\n",
    "\n",
    "1. **Improved Predictive Performance**: Ensembles often achieve higher accuracy and better generalization compared to individual models. They harness the wisdom of multiple models to make more accurate predictions.\n",
    "\n",
    "2. **Reduction of Overfitting**: Combining diverse models helps mitigate overfitting. Errors or biases in one model can be offset by others, leading to a more robust and less prone-to-overfitting model.\n",
    "\n",
    "3. **Enhanced Robustness**: Ensembles are less sensitive to noisy or outlier data points. They can provide more stable predictions in real-world scenarios where data quality may vary.\n",
    "\n",
    "4. **Versatility**: Ensemble methods can be applied to various machine learning algorithms and problem types. This versatility makes them a valuable tool in many different domains.\n",
    "\n",
    "5. **Interpretability**: Some ensemble methods, like Random Forests, offer insights into feature importance, helping understand which features are crucial for making predictions.\n",
    "\n",
    "6. **State-of-the-Art Performance**: Ensembles have consistently delivered top-tier performance in machine learning competitions and real-world applications, making them a go-to choice when high accuracy is required.\n",
    "\n",
    "7. **Handling Complex Relationships**: Ensembles can capture complex patterns and relationships in data by combining different modeling approaches, including both linear and nonlinear methods.\n",
    "\n",
    "8. **Risk Diversification**: In financial or risk modeling, ensembles can help diversify risk by considering multiple models' outputs, which can be crucial for decision-making.\n",
    "\n",
    "9. **Adaptability**: Ensemble techniques can be customized to suit specific needs. For instance, one can create an ensemble with different types of base models or vary the ensemble size to balance complexity and performance.\n",
    "\n",
    "10. **Scalability**: Ensembles can be distributed across multiple machines or processors, making them suitable for large-scale and parallelized machine learning tasks.\n",
    "\n",
    "Overall, ensemble techniques are a powerful tool for improving model performance, reducing overfitting, and increasing the robustness of machine learning models in various applications. However, it's essential to choose the right ensemble method and tune its parameters carefully to achieve the best results for a specific problem.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1183b",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c182a1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ensemble techniques are powerful and often lead to better predictive performance compared to individual models, but they are not guaranteed to be superior in all situations. Whether ensemble techniques are better than individual models depends on several factors:\\n\\n1. **Diversity of Base Models**: The effectiveness of an ensemble often depends on the diversity among its base models. If the base models are very similar or prone to the same types of errors, the ensemble may not provide significant improvements.\\n\\n2. **Quality of Base Models**: The quality of the base models matters. If the individual models are already highly accurate and well-tuned, the marginal gain from combining them into an ensemble may be limited.\\n\\n3. **Dataset Size**: In cases where the dataset is small, creating diverse base models or training a large ensemble may lead to overfitting. In such situations, a single well-regularized model may perform better.\\n\\n4. **Computational Resources**: Ensembles can be computationally expensive, especially if they involve a large number of base models. In resource-constrained environments, using a single model may be more practical.\\n\\n5. **Interpretability**: Individual models are often more interpretable than complex ensembles. If interpretability is a crucial requirement, a single model may be preferred.\\n\\n6. **Domain Knowledge**: Understanding the problem domain and the data is essential. In some cases, domain-specific knowledge may suggest that a particular modeling approach or individual model is more appropriate.\\n\\n7. **Tuning and Complexity**: Ensembles typically require tuning of hyperparameters and may introduce additional complexity. Careful tuning is necessary to ensure that the ensemble performs optimally.\\n\\n8. **Time Sensitivity**: In real-time or low-latency applications, ensembles with many base models may introduce unacceptable delays in making predictions. In such cases, a simpler model may be preferred.\\n\\nIn summary, ensemble techniques are a valuable tool, but their effectiveness depends on the specific problem, data, and resources available. It's essential to experiment with both individual models and ensembles and evaluate their performance using appropriate metrics to determine which approach works best for a given machine learning task.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Ensemble techniques are powerful and often lead to better predictive performance compared to individual models, but they are not guaranteed to be superior in all situations. Whether ensemble techniques are better than individual models depends on several factors:\n",
    "\n",
    "1. **Diversity of Base Models**: The effectiveness of an ensemble often depends on the diversity among its base models. If the base models are very similar or prone to the same types of errors, the ensemble may not provide significant improvements.\n",
    "\n",
    "2. **Quality of Base Models**: The quality of the base models matters. If the individual models are already highly accurate and well-tuned, the marginal gain from combining them into an ensemble may be limited.\n",
    "\n",
    "3. **Dataset Size**: In cases where the dataset is small, creating diverse base models or training a large ensemble may lead to overfitting. In such situations, a single well-regularized model may perform better.\n",
    "\n",
    "4. **Computational Resources**: Ensembles can be computationally expensive, especially if they involve a large number of base models. In resource-constrained environments, using a single model may be more practical.\n",
    "\n",
    "5. **Interpretability**: Individual models are often more interpretable than complex ensembles. If interpretability is a crucial requirement, a single model may be preferred.\n",
    "\n",
    "6. **Domain Knowledge**: Understanding the problem domain and the data is essential. In some cases, domain-specific knowledge may suggest that a particular modeling approach or individual model is more appropriate.\n",
    "\n",
    "7. **Tuning and Complexity**: Ensembles typically require tuning of hyperparameters and may introduce additional complexity. Careful tuning is necessary to ensure that the ensemble performs optimally.\n",
    "\n",
    "8. **Time Sensitivity**: In real-time or low-latency applications, ensembles with many base models may introduce unacceptable delays in making predictions. In such cases, a simpler model may be preferred.\n",
    "\n",
    "In summary, ensemble techniques are a valuable tool, but their effectiveness depends on the specific problem, data, and resources available. It's essential to experiment with both individual models and ensembles and evaluate their performance using appropriate metrics to determine which approach works best for a given machine learning task.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb92c5",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84ceb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate a confidence interval using bootstrap resampling:\\n\\nStart with your data.\\nRepeatedly create random samples (with replacement) from your data.\\nCompute the statistic of interest (e.g., mean) for each sample.\\nSort the computed statistics and find the desired percentiles (e.g., 2.5% and 97.5%) to form the confidence interval.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To calculate a confidence interval using bootstrap resampling:\n",
    "\n",
    "Start with your data.\n",
    "Repeatedly create random samples (with replacement) from your data.\n",
    "Compute the statistic of interest (e.g., mean) for each sample.\n",
    "Sort the computed statistics and find the desired percentiles (e.g., 2.5% and 97.5%) to form the confidence interval.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023fa0ce",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23d24a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ChatGPT\\nBootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data. It allows you to make inferences about population parameters and quantify the uncertainty associated with them. Here are the steps involved in bootstrap:\\n\\nData Collection: Start with your original dataset, which contains observed data points. Let's say you have 'N' data points.\\n\\nRandom Sampling with Replacement: Bootstrap involves randomly selecting 'N' data points from your dataset, allowing for replacement. This means that the same data point can be selected more than once in each bootstrap sample, and some data points may not be selected at all.\\n\\nResampling: Repeat the random sampling process many times (typically thousands or more) to create a set of bootstrap samples. Each bootstrap sample is effectively a new dataset generated by resampling from the original data.\\n\\nStatistic Calculation: For each bootstrap sample, calculate the statistic of interest. This could be any summary statistic, such as the mean, median, variance, or any other parameter you want to estimate.\\n\\nStatistical Estimation: You will end up with a collection of bootstrap statistics, one for each bootstrap sample. This collection approximates the sampling distribution of the statistic you are interested in.\\n\\nConfidence Intervals: From the collection of bootstrap statistics, you can construct confidence intervals. The most common method is to calculate percentiles of the distribution. For instance, a 95% confidence interval can be formed by taking the 2.5th and 97.5th percentiles of the bootstrap statistics.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by \n",
    "repeatedly resampling from the observed data. It allows you to make inferences about population parameters and \n",
    "quantify the uncertainty associated with them. Here are the steps involved in bootstrap:\n",
    "\n",
    "Data Collection: Start with your original dataset, which contains observed data points. Let's say you have 'N' data points.\n",
    "\n",
    "Random Sampling with Replacement: Bootstrap involves randomly selecting 'N' data points from your dataset, allowing for \n",
    "replacement. This means that the same data point can be selected more than once in each bootstrap sample, and some data \n",
    "points may not be selected at all.\n",
    "\n",
    "Resampling: Repeat the random sampling process many times (typically thousands or more) to create a set of bootstrap samples.\n",
    "Each bootstrap sample is effectively a new dataset generated by resampling from the original data.\n",
    "\n",
    "Statistic Calculation: For each bootstrap sample, calculate the statistic of interest. This could be any summary statistic,\n",
    "such as the mean, median, variance, or any other parameter you want to estimate.\n",
    "\n",
    "Statistical Estimation: You will end up with a collection of bootstrap statistics, one for each bootstrap sample. \n",
    "This collection approximates the sampling distribution of the statistic you are interested in.\n",
    "\n",
    "Confidence Intervals: From the collection of bootstrap statistics, you can construct confidence intervals. \n",
    "The most common method is to calculate percentiles of the distribution. For instance, a 95% confidence interval\n",
    "can be formed by taking the 2.5th and 97.5th percentiles of the bootstrap statistics.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e86e1",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823f418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean Height: [14.44227864 15.5489474 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "sample_mean = 15  # meters\n",
    "sample_std = 2    # meters\n",
    "sample_size = 50\n",
    "\n",
    "# Number of bootstrap resamples\n",
    "num_resamples = 10000\n",
    "\n",
    "# Placeholder for bootstrap sample means\n",
    "bootstrap_means = []\n",
    "\n",
    "# Create a bootstrap distribution\n",
    "for _ in range(num_resamples):\n",
    "    # Generate a bootstrap sample by resampling with replacement\n",
    "    bootstrap_sample = np.random.normal(loc=sample_mean, scale=sample_std, size=sample_size)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    \n",
    "    # Store the bootstrap sample mean\n",
    "    bootstrap_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c7b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
