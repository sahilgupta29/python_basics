{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424dec74",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4154ce8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nK-Nearest Neighbors (KNN) is a simple and versatile machine learning algorithm used for \\nclassification and regression tasks. In KNN, the class or value of a data point is determined by the class or values\\nof its nearest neighbors in the training data. It's based on the principle that similar data points tend to have similar outcomes.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "K-Nearest Neighbors (KNN) is a simple and versatile machine learning algorithm used for \n",
    "classification and regression tasks. In KNN, the class or value of a data point is determined by the class or values\n",
    "of its nearest neighbors in the training data. It's based on the principle that similar data points tend to have similar outcomes.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda2a2",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c666128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo choose the value of K in K-Nearest Neighbors (KNN), you typically use techniques like GridSearchCV to systematically test different values of K\\nand evaluate their performance on a validation set. The optimal K value is often found by selecting the one that results in the best balance between \\nmodel bias (low K) and model variance (high K) for your specific dataset.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To choose the value of K in K-Nearest Neighbors (KNN), you typically use techniques like GridSearchCV to systematically test different values of K\n",
    "and evaluate their performance on a validation set. The optimal K value is often found by selecting the one that results in the best balance between \n",
    "model bias (low K) and model variance (high K) for your specific dataset.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d314f3f",
   "metadata": {},
   "source": [
    "\"\"\"Q3. What is the difference between KNN classifier and KNN regressor?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d76d240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main difference between KNN classifier and KNN regressor is in their output:\\n\\n1. **KNN Classifier**: It is used for classification tasks and predicts a class label for a data point based on the majority \\nclass among its k-nearest neighbors.\\n\\n2. **KNN Regressor**: It is used for regression tasks and predicts a continuous numerical value for a data point based on the\\naverage (or another measure) of the values among its k-nearest neighbors.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The main difference between KNN classifier and KNN regressor is in their output:\n",
    "\n",
    "1. **KNN Classifier**: It is used for classification tasks and predicts a class label for a data point based on the majority \n",
    "class among its k-nearest neighbors.\n",
    "\n",
    "2. **KNN Regressor**: It is used for regression tasks and predicts a continuous numerical value for a data point based on the\n",
    "average (or another measure) of the values among its k-nearest neighbors.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed511adb",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fea1df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To measure the performance of a K-Nearest Neighbors (KNN) model, you can use various evaluation metrics depending on whether it's a classification or regression problem:\\n\\n**For KNN Classification:**\\n\\n1. **Accuracy**: The proportion of correctly classified instances.\\n\\n2. **Confusion Matrix**: Breakdown of correct and incorrect predictions, showing true positives, true negatives, false positives, and false negatives.\\n\\n3. **Precision and Recall**: Measures of the model's ability to make correct positive predictions and find all actual positives, respectively.\\n\\n4. **F1-Score**: The harmonic mean of precision and recall, providing a balanced measure of a model's performance.\\n\\n5. **ROC Curve and AUC**: Useful for binary classification tasks, it plots the trade-off between true positive rate and false positive rate.\\n\\n**For KNN Regression:**\\n\\n1. **Mean Absolute Error (MAE)**: The average of the absolute differences between the predicted and actual values.\\n\\n2. **Mean Squared Error (MSE)**: The average of the squared differences between the predicted and actual values.\\n\\n3. **Root Mean Squared Error (RMSE)**: The square root of the MSE, giving you an error measure in the same unit as the target variable.\\n\\n4. **R-squared (R2)**: Measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit.\\n\\nThe choice of metric depends on the specific problem and what you want to prioritize. It's often a good practice to consider multiple metrics to get a comprehensive understanding of the model's performance.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"To measure the performance of a K-Nearest Neighbors (KNN) model, you can use various evaluation metrics depending on whether \n",
    "it's a classification or regression problem:\n",
    "\n",
    "**For KNN Classification:**\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances.\n",
    "\n",
    "2. **Confusion Matrix**: Breakdown of correct and incorrect predictions, showing true positives, true negatives, \n",
    "false positives, and false negatives.\n",
    "\n",
    "3. **Precision and Recall**: Measures of the model's ability to make correct positive predictions and find all actual \n",
    "positives, respectively.\n",
    "\n",
    "4. **F1-Score**: The harmonic mean of precision and recall, providing a balanced measure of a model's performance.\n",
    "\n",
    "5. **ROC Curve and AUC**: Useful for binary classification tasks, it plots the trade-off between true positive rate and \n",
    "false positive rate.\n",
    "\n",
    "**For KNN Regression:**\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: The average of the absolute differences between the predicted and actual values.\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: The average of the squared differences between the predicted and actual values.\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**: The square root of the MSE, giving you an error measure in the same unit as the\n",
    "target variable.\n",
    "\n",
    "4. **R-squared (R2)**: Measures the proportion of the variance in the dependent variable that is predictable from the \n",
    "independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit.\n",
    "\n",
    "The choice of metric depends on the specific problem and what you want to prioritize. It's often a good practice to \n",
    "consider multiple metrics to get a comprehensive understanding of the model's performance.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25bb098",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26109958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The \"Curse of Dimensionality\" in K-Nearest Neighbors (KNN) refers to the challenges and issues that arise when working\\nwith high-dimensional data. It primarily affects the performance and efficiency of the KNN algorithm. \\nHere\\'s a brief explanation:\\n\\n1. **Increased Computation**: As the number of dimensions (features) in the dataset increases, the computational cost of\\nfinding nearest neighbors grows exponentially. This is because the search space becomes much larger and more sparse,\\nrequiring more distance calculations.\\n\\n2. **Reduced Discriminative Power**: In high-dimensional spaces, data points tend to be far apart from each other,\\nwhich can make it challenging to determine meaningful nearest neighbors. The notion of \"closeness\" loses its \\ndiscriminatory power.\\n\\n3. **Overfitting**: KNN can become prone to overfitting in high-dimensional spaces because it might start capturing \\nnoise or random variations in the data rather than genuine patterns.\\n\\n4. **Increased Data Requirement**: More data is needed to make KNN effective in high-dimensional spaces, which can be \\nimpractical in many real-world scenarios.\\n\\nTo mitigate the Curse of Dimensionality in KNN, you can consider dimensionality reduction techniques \\n(e.g., Principal Component Analysis) or feature selection to reduce the number of dimensions and improve the \\nalgorithm\\'s performance. Additionally, using distance metrics that are less sensitive to high-dimensional spaces (e.g., cosine similarity) can also help.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The \"Curse of Dimensionality\" in K-Nearest Neighbors (KNN) refers to the challenges and issues that arise when working\n",
    "with high-dimensional data. It primarily affects the performance and efficiency of the KNN algorithm. \n",
    "Here's a brief explanation:\n",
    "\n",
    "1. **Increased Computation**: As the number of dimensions (features) in the dataset increases, the computational cost of\n",
    "finding nearest neighbors grows exponentially. This is because the search space becomes much larger and more sparse,\n",
    "requiring more distance calculations.\n",
    "\n",
    "2. **Reduced Discriminative Power**: In high-dimensional spaces, data points tend to be far apart from each other,\n",
    "which can make it challenging to determine meaningful nearest neighbors. The notion of \"closeness\" loses its \n",
    "discriminatory power.\n",
    "\n",
    "3. **Overfitting**: KNN can become prone to overfitting in high-dimensional spaces because it might start capturing \n",
    "noise or random variations in the data rather than genuine patterns.\n",
    "\n",
    "4. **Increased Data Requirement**: More data is needed to make KNN effective in high-dimensional spaces, which can be \n",
    "impractical in many real-world scenarios.\n",
    "\n",
    "To mitigate the Curse of Dimensionality in KNN, you can consider dimensionality reduction techniques \n",
    "(e.g., Principal Component Analysis) or feature selection to reduce the number of dimensions and improve the \n",
    "algorithm's performance. Additionally, using distance metrics that are less sensitive to high-dimensional spaces (e.g., cosine similarity) can also help.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5573c7",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9e602a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Handling missing values in K-Nearest Neighbors (KNN):\\n\\n1. **Imputation**: Fill missing values with reasonable estimates (e.g., mean, median, mode) based on available data.\\n\\n2. **Data Transformation**: Use techniques like mean imputation or regression to estimate missing values during the distance calculation phase.\\n\\n3. **KNN Imputation**: Predict missing values for a data point based on the values of its k-nearest neighbors.\\n\\n4. **Weighted KNN**: Give more weight to the nearest neighbors with non-missing values when imputing missing values for a data point.\\n\\n5. **Consider removing instances**: In some cases, it may be appropriate to remove data instances with missing values if they don't constitute a significant portion of the dataset.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Handling missing values in K-Nearest Neighbors (KNN):\n",
    "\n",
    "1. **Imputation**: Fill missing values with reasonable estimates (e.g., mean, median, mode) based on available data.\n",
    "\n",
    "2. **Data Transformation**: Use techniques like mean imputation or regression to estimate missing values during the distance calculation phase.\n",
    "\n",
    "3. **KNN Imputation**: Predict missing values for a data point based on the values of its k-nearest neighbors.\n",
    "\n",
    "4. **Weighted KNN**: Give more weight to the nearest neighbors with non-missing values when imputing missing values for a data point.\n",
    "\n",
    "5. **Consider removing instances**: In some cases, it may be appropriate to remove data instances with missing values if they don't constitute a significant portion of the dataset.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e05db2",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c451db65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"K-Nearest Neighbors (KNN) Classifier and Regressor have different purposes and are suited for different types of problems. Here's a comparison and contrast of their performance and use cases:\\n\\n**KNN Classifier**:\\n\\n1. **Purpose**: KNN Classifier is used for classification tasks where the goal is to predict discrete class labels. For example, it can be used for image classification, spam detection, or sentiment analysis.\\n\\n2. **Output**: It provides class labels as output, typically integers or categorical values.\\n\\n3. **Evaluation Metrics**: Performance is evaluated using classification metrics like accuracy, precision, recall, F1-score, etc.\\n\\n4. **Use Cases**: KNN classification is suitable for problems where you want to assign data points to predefined categories or classes.\\n\\n**KNN Regressor**:\\n\\n1. **Purpose**: KNN Regressor is used for regression tasks where the goal is to predict continuous numerical values. For example, it can be used for predicting house prices, stock prices, or temperature forecasting.\\n\\n2. **Output**: It provides continuous numerical values as output.\\n\\n3. **Evaluation Metrics**: Performance is evaluated using regression metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared.\\n\\n4. **Use Cases**: KNN regression is suitable for problems where you need to estimate a numeric target value based on the values of its neighbors.\\n\\n**Comparison**:\\n\\n- KNN Classifier and Regressor both rely on the concept of proximity to make predictions. They look for similarity among data points.\\n\\n- KNN Classifier is designed for classification tasks, while KNN Regressor is designed for regression tasks. They have different objectives and output types.\\n\\n- The choice between them depends on the nature of the problem and the type of output you want. If your problem involves predicting categories or classes, use KNN Classifier. If it involves predicting numeric values, use KNN Regressor.\\n\\n**Which to Choose**:\\n\\n- Choose KNN Classifier for problems like image classification, text classification, and sentiment analysis.\\n\\n- Choose KNN Regressor for problems like predicting house prices, stock prices, and any task where you need to estimate a numeric value.\\n\\nThe choice should be based on the problem's nature and the type of prediction required.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"K-Nearest Neighbors (KNN) Classifier and Regressor have different purposes and are suited for different types of problems. Here's a comparison and contrast of their performance and use cases:\n",
    "\n",
    "**KNN Classifier**:\n",
    "\n",
    "1. **Purpose**: KNN Classifier is used for classification tasks where the goal is to predict discrete class labels. For example, it can be used for image classification, spam detection, or sentiment analysis.\n",
    "\n",
    "2. **Output**: It provides class labels as output, typically integers or categorical values.\n",
    "\n",
    "3. **Evaluation Metrics**: Performance is evaluated using classification metrics like accuracy, precision, recall, F1-score, etc.\n",
    "\n",
    "4. **Use Cases**: KNN classification is suitable for problems where you want to assign data points to predefined categories or classes.\n",
    "\n",
    "**KNN Regressor**:\n",
    "\n",
    "1. **Purpose**: KNN Regressor is used for regression tasks where the goal is to predict continuous numerical values. For example, it can be used for predicting house prices, stock prices, or temperature forecasting.\n",
    "\n",
    "2. **Output**: It provides continuous numerical values as output.\n",
    "\n",
    "3. **Evaluation Metrics**: Performance is evaluated using regression metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared.\n",
    "\n",
    "4. **Use Cases**: KNN regression is suitable for problems where you need to estimate a numeric target value based on the values of its neighbors.\n",
    "\n",
    "**Comparison**:\n",
    "\n",
    "- KNN Classifier and Regressor both rely on the concept of proximity to make predictions. They look for similarity among data points.\n",
    "\n",
    "- KNN Classifier is designed for classification tasks, while KNN Regressor is designed for regression tasks. They have different objectives and output types.\n",
    "\n",
    "- The choice between them depends on the nature of the problem and the type of output you want. If your problem involves predicting categories or classes, use KNN Classifier. If it involves predicting numeric values, use KNN Regressor.\n",
    "\n",
    "**Which to Choose**:\n",
    "\n",
    "- Choose KNN Classifier for problems like image classification, text classification, and sentiment analysis.\n",
    "\n",
    "- Choose KNN Regressor for problems like predicting house prices, stock prices, and any task where you need to estimate a numeric value.\n",
    "\n",
    "The choice should be based on the problem's nature and the type of prediction required.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c414074",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5a841d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"K-Nearest Neighbors (KNN) is a simple and intuitive algorithm, but it comes with its strengths and weaknesses, both for classification and regression tasks. Here's an overview:\\n\\n**Strengths of KNN:**\\n\\n*For Classification:*\\n\\n1. **Simplicity**: KNN is easy to understand and implement, making it a good choice for quick, initial models.\\n2. **Non-parametric**: KNN doesn't make strong assumptions about the underlying data distribution.\\n3. **Adaptability**: It can work well with non-linear data and complex decision boundaries.\\n\\n*For Regression:*\\n\\n1. **Non-parametric**: Like in classification, KNN's non-parametric nature allows it to handle data without making strong assumptions about the relationship between features and the target variable.\\n2. **Adaptability**: KNN can work well when relationships between predictors and the target variable are not linear.\\n\\n**Weaknesses of KNN:**\\n\\n*For Classification:*\\n\\n1. **Computationally Intensive**: KNN can be slow and resource-intensive, particularly when working with large datasets or many features.\\n2. **Sensitivity to Irrelevant Features**: It can be sensitive to irrelevant features, leading to suboptimal results.\\n\\n*For Regression:*\\n\\n1. **Prediction Variability**: KNN regression can produce noisy predictions, particularly when the dataset is small or the number of neighbors (k) is large.\\n2. **Sensitivity to Outliers**: Outliers in the dataset can have a significant impact on the predictions.\\n\\n**Addressing Weaknesses:**\\n\\n1. **Feature Selection/Engineering**: Reduce irrelevant features or engineer new, more relevant features to improve the performance of KNN.\\n2. **Normalization/Standardization**: Scale or standardize features to ensure that they contribute equally to distance calculations.\\n3. **Cross-Validation**: Use cross-validation to find the optimal value of 'k' and mitigate overfitting.\\n4. **Distance Metrics**: Experiment with different distance metrics (e.g., Euclidean, Manhattan, cosine similarity) to find the one that works best for your data.\\n5. **Weighted KNN**: Implement weighted KNN to give more importance to closer neighbors during prediction.\\n6. **Parallelization and Optimization**: For large datasets, consider parallelizing the algorithm or using specialized data structures (e.g., KD-trees or Ball trees) to speed up search.\\n\\nIn summary, KNN is a versatile algorithm with its own set of advantages and drawbacks. Addressing its weaknesses often involves preprocessing steps, careful choice of parameters, and, in some cases, alternative distance metrics or data structures. The suitability of KNN for a particular task depends on the characteristics of the data and the desired outcome.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"K-Nearest Neighbors (KNN) is a simple and intuitive algorithm, but it comes with its strengths and weaknesses, both for classification and regression tasks. Here's an overview:\n",
    "\n",
    "**Strengths of KNN:**\n",
    "\n",
    "*For Classification:*\n",
    "\n",
    "1. **Simplicity**: KNN is easy to understand and implement, making it a good choice for quick, initial models.\n",
    "2. **Non-parametric**: KNN doesn't make strong assumptions about the underlying data distribution.\n",
    "3. **Adaptability**: It can work well with non-linear data and complex decision boundaries.\n",
    "\n",
    "*For Regression:*\n",
    "\n",
    "1. **Non-parametric**: Like in classification, KNN's non-parametric nature allows it to handle data without making strong assumptions about the relationship between features and the target variable.\n",
    "2. **Adaptability**: KNN can work well when relationships between predictors and the target variable are not linear.\n",
    "\n",
    "**Weaknesses of KNN:**\n",
    "\n",
    "*For Classification:*\n",
    "\n",
    "1. **Computationally Intensive**: KNN can be slow and resource-intensive, particularly when working with large datasets or many features.\n",
    "2. **Sensitivity to Irrelevant Features**: It can be sensitive to irrelevant features, leading to suboptimal results.\n",
    "\n",
    "*For Regression:*\n",
    "\n",
    "1. **Prediction Variability**: KNN regression can produce noisy predictions, particularly when the dataset is small or the number of neighbors (k) is large.\n",
    "2. **Sensitivity to Outliers**: Outliers in the dataset can have a significant impact on the predictions.\n",
    "\n",
    "**Addressing Weaknesses:**\n",
    "\n",
    "1. **Feature Selection/Engineering**: Reduce irrelevant features or engineer new, more relevant features to improve the performance of KNN.\n",
    "2. **Normalization/Standardization**: Scale or standardize features to ensure that they contribute equally to distance calculations.\n",
    "3. **Cross-Validation**: Use cross-validation to find the optimal value of 'k' and mitigate overfitting.\n",
    "4. **Distance Metrics**: Experiment with different distance metrics (e.g., Euclidean, Manhattan, cosine similarity) to find the one that works best for your data.\n",
    "5. **Weighted KNN**: Implement weighted KNN to give more importance to closer neighbors during prediction.\n",
    "6. **Parallelization and Optimization**: For large datasets, consider parallelizing the algorithm or using specialized data structures (e.g., KD-trees or Ball trees) to speed up search.\n",
    "\n",
    "In summary, KNN is a versatile algorithm with its own set of advantages and drawbacks. Addressing its weaknesses often involves preprocessing steps, careful choice of parameters, and, in some cases, alternative distance metrics or data structures. The suitability of KNN for a particular task depends on the characteristics of the data and the desired outcome.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df323e",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b906eb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main difference between Euclidean distance and Manhattan distance in K-Nearest Neighbors (KNN) is how they measure distance between data points:\\n\\n1. **Euclidean Distance**: It calculates the straight-line distance (as the crow flies) between two points, considering the square root of the sum of squared differences in each dimension. It is more sensitive to differences in feature values and is suitable for problems where a diagonal path makes sense, such as in continuous spaces.\\n\\n2. **Manhattan Distance**: It calculates the distance as the sum of the absolute differences between the coordinates of two points, often measured along orthogonal axes (horizontal and vertical). Manhattan distance is less sensitive to outliers and works well when you want to measure distance along grid-like paths.\\n\\nThe choice between Euclidean and Manhattan distance depends on the problem and the nature of the data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The main difference between Euclidean distance and Manhattan distance in K-Nearest Neighbors (KNN) is how they measure distance between data points:\n",
    "\n",
    "1. **Euclidean Distance**: It calculates the straight-line distance (as the crow flies) between two points, considering the square root of the sum of squared differences in each dimension. It is more sensitive to differences in feature values and is suitable for problems where a diagonal path makes sense, such as in continuous spaces.\n",
    "\n",
    "2. **Manhattan Distance**: It calculates the distance as the sum of the absolute differences between the coordinates of two points, often measured along orthogonal axes (horizontal and vertical). Manhattan distance is less sensitive to outliers and works well when you want to measure distance along grid-like paths.\n",
    "\n",
    "The choice between Euclidean and Manhattan distance depends on the problem and the nature of the data.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8fad1",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133b84a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The role of feature scaling in K-Nearest Neighbors (KNN) is to ensure that all features contribute equally to the distance calculation between data points. Feature scaling is essential because KNN relies on measuring the distance or similarity between data points, and features with larger scales can disproportionately influence this calculation. Without feature scaling, KNN may give too much weight to features with larger numeric values, leading to biased results.\\n\\nFeature scaling typically involves two common techniques:\\n\\n1. **Normalization (Min-Max Scaling)**: This method scales features to a common range, usually between 0 and 1. It preserves the relationships between data points and is suitable when the distribution of data is not Gaussian. The formula for Min-Max scaling is:\\n\\n   ```\\n   X_normalized = (X - X_min) / (X_max - X_min)\\n   ```|\\n\\n2. **Standardization (Z-score Scaling)**: Standardization transforms features to have a mean of 0 and a standard deviation of 1. It is more robust to outliers and is suitable when the data follows a Gaussian distribution. The formula for standardization is:\\n\\n   ```\\n   X_standardized = (X - X_mean) / X_std\\n   ```\\n\\nBy applying feature scaling, you ensure that all features have similar scales, making KNN more effective in measuring distances and finding the nearest neighbors based on a balanced contribution of all features.|'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"The role of feature scaling in K-Nearest Neighbors (KNN) is to ensure that all features contribute equally to the distance calculation between data points. Feature scaling is essential because KNN relies on measuring the distance or similarity between data points, and features with larger scales can disproportionately influence this calculation. Without feature scaling, KNN may give too much weight to features with larger numeric values, leading to biased results.\n",
    "\n",
    "Feature scaling typically involves two common techniques:\n",
    "\n",
    "1. **Normalization (Min-Max Scaling)**: This method scales features to a common range, usually between 0 and 1. It preserves the relationships between data points and is suitable when the distribution of data is not Gaussian. The formula for Min-Max scaling is:\n",
    "\n",
    "   ```\n",
    "   X_normalized = (X - X_min) / (X_max - X_min)\n",
    "   ```|\n",
    "\n",
    "2. **Standardization (Z-score Scaling)**: Standardization transforms features to have a mean of 0 and a standard deviation of 1. It is more robust to outliers and is suitable when the data follows a Gaussian distribution. The formula for standardization is:\n",
    "\n",
    "   ```\n",
    "   X_standardized = (X - X_mean) / X_std\n",
    "   ```\n",
    "\n",
    "By applying feature scaling, you ensure that all features have similar scales, making KNN more effective in measuring distances and finding the nearest neighbors based on a balanced contribution of all features.|\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93afbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Complete'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Complete\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e9086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
